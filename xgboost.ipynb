{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c971d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71bc96ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"data/0527_final.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d70f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns to use\n",
    "df.columns == ['year', 'IGFC', 'gvkey', 'coname', 'cusip', 'co_per_rol', 'forced_3', 'departure_code', 'forced_3_4', 'tenure', 'I3y_lag', 'sic', 'ff17_code', 'execid', \n",
    "               'gender', 'page', 'becameceo', 'leftofc', 'roa', 'gross_margin', 'operating_margin', 'asset_turnover', 'receivables_turnover', 'inventory_turnover', 'debt_to_assets', 'equity_ratio', 'debt_to_equity', 'current_ratio', 'quick_ratio', 'working_capital', 'ocf_to_assets', 'ocf_to_sales', \n",
    "               'ebit_to_assets', 'ebitda_to_assets', 'pi_to_assets', 'ib_to_assets', 'txt_to_assets', 'xint_to_assets', 'capx_to_assets', 'dp_to_assets', 'emp_to_assets', 'tdc1_to_assets', 'bonus_to_assets', 'ppent_to_assets', 're_to_assets', 'rect_to_assets', 'salary_to_assets', 'option_to_asset', \n",
    "               'roa_3y_lag', 'gross_margin_3y_lag', 'operating_margin_3y_lag', 'asset_turnover_3y_lag', 'receivables_turnover_3y_lag', 'inventory_turnover_3y_lag', 'debt_to_assets_3y_lag', 'equity_ratio_3y_lag', 'debt_to_equity_3y_lag', 'current_ratio_3y_lag', 'quick_ratio_3y_lag', 'working_capital_3y_lag', 'ocf_to_assets_3y_lag', 'ocf_to_sales_3y_lag', \n",
    "               'ebit_to_assets_3y_lag', 'ebitda_to_assets_3y_lag', 'pi_to_assets_3y_lag', 'ib_to_assets_3y_lag', 'txt_to_assets_3y_lag', 'xint_to_assets_3y_lag', 'capx_to_assets_3y_lag', 'dp_to_assets_3y_lag', 'emp_to_assets_3y_lag', 'tdc1_to_assets_3y_lag', 'bonus_to_assets_3y_lag', 'ppent_to_assets_3y_lag', 're_to_assets_3y_lag', 'rect_to_assets_3y_lag', 'salary_to_assets_3y_lag', 'option_to_asset_3y_lag', \n",
    "               'Iff1', 'Iff2', 'Iff3', 'Iff4', 'Iff5', 'Iff6', 'Iff7', 'Iff8', 'Iff9', 'Iff10', 'Iff11', 'Iff12', 'Iff13', 'Iff14', 'Iff15', 'Iff16', 'Iff17', \n",
    "               'I1992', 'I1993', 'I1994', 'I1995', 'I1996', 'I1997', 'I1998', 'I1999', 'I2000', 'I2001', 'I2002', 'I2003', 'I2004', 'I2005', 'I2006', 'I2007', 'I2008', 'I2009', 'I2010', 'I2011', 'I2012', 'I2013', 'I2014', 'I2015', 'I2016', 'I2017', 'I2018', 'I2019', 'I2020', 'I2021', 'I2022']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becd84b2",
   "metadata": {},
   "source": [
    "# boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e89c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np # import numpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "\n",
    "def var_select_boosting(\n",
    "    df: pd.DataFrame,\n",
    "    k: int,\n",
    "    X: list,\n",
    "    y: str,\n",
    "    start: int = 1992,\n",
    "    end: int = 2022,\n",
    "    boosting: str = 'xgb',\n",
    "    year_col: str = None,\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = 42\n",
    "):\n",
    "    # (Omitted sections identical to previous code)\n",
    "    feature_cols = X\n",
    "    target_col = y\n",
    "    boosting_method = boosting.lower()\n",
    "\n",
    "    # --- 1. Input Validation ---\n",
    "    # (Same as before)\n",
    "    if boosting_method not in ['ada', 'xgb', 'lgbm']:\n",
    "        raise ValueError(\"boosting_method must be one of 'ada', 'xgb', or 'lgbm'.\")\n",
    "    if target_col not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_col}' not found in the DataFrame.\")\n",
    "    missing_features = [col for col in feature_cols if col not in df.columns]\n",
    "    if missing_features:\n",
    "        raise ValueError(f\"Feature columns '{missing_features}' not found in the DataFrame.\")\n",
    "    if not isinstance(k, int) or k <= 0:\n",
    "        raise ValueError(\"k must be a positive integer.\")\n",
    "    if k > len(feature_cols):\n",
    "        warnings.warn(f\"k ({k}) is larger than the number of available features ({len(feature_cols)}). Selecting all features.\")\n",
    "        k = len(feature_cols)\n",
    "\n",
    "    # --- 2. Year Filtering (Optional) ---\n",
    "    df_to_use = df.copy()\n",
    "    if year_col:\n",
    "        if year_col not in df_to_use.columns:\n",
    "            raise ValueError(f\"Year column '{year_col}' not found in the DataFrame.\")\n",
    "        print(f\"Filtering data for years {start}-{end} using column '{year_col}'.\")\n",
    "        try:\n",
    "            df_to_use[year_col] = pd.to_numeric(df_to_use[year_col], errors='raise')\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Cannot convert year column '{year_col}' to numeric. Error: {e}\")\n",
    "        \n",
    "        df_to_use = df_to_use[(df_to_use[year_col] >= start) & (df_to_use[year_col] <= end)]\n",
    "        if df_to_use.empty:\n",
    "            raise ValueError(f\"No data found for the specified year range ({start}-{end}).\")\n",
    "    else:\n",
    "        print(\"Note: 'year_col' not specified. Using the entire DataFrame.\")\n",
    "\n",
    "    if df_to_use.empty:\n",
    "        raise ValueError(\"DataFrame is empty after filtering.\")\n",
    "\n",
    "    # --- 3. Data Preparation (X_data, y_data) and Conditional NaN/inf Handling ---\n",
    "    X_data_original = df_to_use[feature_cols]\n",
    "    y_processed = df_to_use[target_col].copy() # Assuming y is numeric and has no NaNs\n",
    "\n",
    "    X_data = X_data_original.copy()\n",
    "    \n",
    "    print(f\"Number of rows in X_data/y_processed before preparation: {len(X_data)}\")\n",
    "\n",
    "    # 1. Unify inf values to NaN in X_data for all boosting methods\n",
    "    # Can use .to_numpy() for efficient check or DataFrame's replace method\n",
    "    inf_present_in_X = np.isinf(X_data.select_dtypes(include=np.number).to_numpy()).any()\n",
    "    if inf_present_in_X:\n",
    "        print(\"Converting inf values in feature data (X_data) to NaN.\")\n",
    "        X_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    if boosting_method in ['ada']:\n",
    "        print(f\"When using '{boosting_method.upper()}', rows with NaN in feature data (X_data) will be removed (inf is handled after being converted to NaN).\")\n",
    "        # Now, NaNs in X_data are either original NaNs or NaNs converted from inf\n",
    "        nan_in_X_mask = X_data.isnull().any(axis=1)\n",
    "        \n",
    "        if nan_in_X_mask.any(): \n",
    "            X_data_before_drop_len = len(X_data)\n",
    "            X_data = X_data[~nan_in_X_mask]\n",
    "            y_processed = y_processed[~nan_in_X_mask] # Filter y_processed accordingly\n",
    "\n",
    "            rows_dropped_count = X_data_before_drop_len - len(X_data)\n",
    "            if rows_dropped_count > 0:\n",
    "                 print(f\"Removed {rows_dropped_count} rows containing NaN (or former inf) in feature data (X_data).\")\n",
    "        else:\n",
    "            if inf_present_in_X: # inf was present but after conversion to NaN, no other NaNs existed\n",
    "                 print(\"inf in feature data (X_data) was converted to NaN, but no rows were removed as there were no additional NaNs.\")\n",
    "            else: # No NaN/inf to begin with\n",
    "                 print(\"No rows were removed as there were no NaN/inf values in the feature data (X_data).\")\n",
    "        \n",
    "        if X_data.empty: \n",
    "            raise ValueError(\"No data remains after removing NaN/inf rows from feature data (X_data).\")\n",
    "    else: # e.g., 'lgbm', 'xgb'\n",
    "        print(f\"When using '{boosting_method.upper()}', NaNs in feature data (X_data) (inf is converted to NaN) are handled internally, so rows are not removed.\")\n",
    "        if X_data.isnull().any().any(): # If NaNs actually exist (original or converted from inf)\n",
    "            print(\"Note: Feature data (X_data) contains NaN values (some converted from former inf), but the model handles them internally.\")\n",
    "        elif inf_present_in_X: # Only inf was converted to NaN, and no other NaNs exist\n",
    "             print(\"Note: inf values in feature data (X_data) have been converted to NaN. The model handles NaNs internally.\")\n",
    "\n",
    "    print(f\"Final number of rows in X_data/y_processed: {len(X_data)}\")\n",
    "\n",
    "    # Assuming y_processed is numeric with no NaNs (user should verify)\n",
    "    if not pd.api.types.is_numeric_dtype(y_processed):\n",
    "        warnings.warn(f\"Warning: Target variable '{target_col}' was assumed to be numeric, but its actual type is {y_processed.dtype}.\")\n",
    "    if y_processed.isnull().any():\n",
    "         warnings.warn(f\"Warning: Target variable '{target_col}' was assumed to have no NaNs, but NaN values were found.\")\n",
    "\n",
    "    # --- 4. Feature Type Check Based on Model ---\n",
    "    # (Same as before)\n",
    "    if boosting_method in ['ada']:\n",
    "        non_numeric_cols = X_data.select_dtypes(exclude=np.number).columns\n",
    "        if len(non_numeric_cols) > 0:\n",
    "            raise ValueError(\n",
    "                f\"{boosting_method.upper()} model requires all feature variables to be numeric. \"\n",
    "                f\"The following non-numeric columns were found: {list(non_numeric_cols)}. \"\n",
    "                \"Please perform preprocessing such as Label Encoding or One-Hot Encoding.\"\n",
    "            )\n",
    "    elif boosting_method == 'lgbm':\n",
    "        for col in X_data.select_dtypes(include='object').columns:\n",
    "            print(f\"For LightGBM, converting object type column '{col}' to 'category' type.\")\n",
    "            X_data.loc[:, col] = X_data[col].astype('category')\n",
    "\n",
    "    # --- 5. Data Splitting (Train/Test) ---\n",
    "    # (Same as before)\n",
    "    if not isinstance(y_processed, pd.Series): # Check if y_processed is a Series\n",
    "        y_processed = pd.Series(y_processed, name=target_col, index=X_data.index if len(y_processed) == len(X_data) else None)\n",
    "\n",
    "    is_classification_target = y_processed.nunique() > 1 and y_processed.nunique() < len(y_processed) * 0.8\n",
    "    stratify_option = y_processed if is_classification_target else None\n",
    "    if stratify_option is not None:\n",
    "        class_counts = y_processed.value_counts()\n",
    "        if (class_counts < 2).any(): \n",
    "            warnings.warn(\"Stratified sampling is not possible because some classes in the target variable have only one sample. Proceeding with regular sampling.\")\n",
    "            stratify_option = None\n",
    "            \n",
    "    try:\n",
    "        X_train, X_test, y_train_np, y_test_np = train_test_split(\n",
    "            X_data, y_processed, test_size=test_size, random_state=random_state, stratify=stratify_option\n",
    "        )\n",
    "    except ValueError as e:\n",
    "        warnings.warn(f\"Error during data splitting (likely related to stratify): {e}. Retrying with regular splitting.\")\n",
    "        X_train, X_test, y_train_np, y_test_np = train_test_split(\n",
    "            X_data, y_processed, test_size=test_size, random_state=random_state, stratify=None\n",
    "        )\n",
    "\n",
    "    y_train = pd.Series(y_train_np, name=target_col, index=X_train.index)\n",
    "    y_test = pd.Series(y_test_np, name=target_col, index=X_test.index)\n",
    "    \n",
    "    # --- 6. Initialize and Train Boosting Model ---\n",
    "    # (Same as before)\n",
    "    model = None\n",
    "    if boosting_method == 'ada':\n",
    "        model = AdaBoostClassifier(random_state=random_state)\n",
    "    elif boosting_method == 'xgb':\n",
    "        xgb_params = {'random_state': random_state}\n",
    "        model = XGBClassifier(**xgb_params)\n",
    "    elif boosting_method == 'lgbm':\n",
    "        model = LGBMClassifier(random_state=random_state, verbosity=-1)\n",
    "\n",
    "    model.fit(X_train, y_train) \n",
    "\n",
    "    # --- 7. Extract Feature Importances ---\n",
    "    # (Same as before)\n",
    "    importances = model.feature_importances_\n",
    "\n",
    "    # --- 8. Select Top K Features ---\n",
    "    # (Same as before)\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': importances\n",
    "    })\n",
    "    top_k_features_df = feature_importance_df.sort_values(by='importance', ascending=False).head(k)\n",
    "    selected_feature_names = top_k_features_df['feature'].tolist()\n",
    "\n",
    "    # --- 9. Reconstruct X Datasets with Selected Features ---\n",
    "    # (Same as before)\n",
    "    X_train_selected = X_train[selected_feature_names]\n",
    "    X_test_selected = X_test[selected_feature_names]\n",
    "\n",
    "    # --- 10. Visualize Feature Importances ---\n",
    "    # (Same as before)\n",
    "    fig, ax = plt.subplots(figsize=(12, max(6, k * 0.55)))\n",
    "    sns.barplot(x='importance', y='feature', data=top_k_features_df, ax=ax, palette=\"plasma\")\n",
    "    ax.set_title(f'\"{boosting_method.upper()}\" model top {k} importance variables', fontsize=16)\n",
    "    ax.set_xlabel('Importance', fontsize=14)\n",
    "    ax.set_ylabel('Feature', fontsize=14)\n",
    "    ax.tick_params(axis='x', labelsize=12)\n",
    "    ax.tick_params(axis='y', labelsize=12)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return top_k_features_df, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "705f8b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_cols = ['gender', 'page', 'tenure', 'salary_to_assets', 'emp_to_assets', 'tdc1_to_assets', 'bonus_to_assets','option_to_asset']\n",
    "firm_cols = ['ebit_to_assets', 'ebitda_to_assets', 'pi_to_assets', 'ib_to_assets', 'txt_to_assets', 'xint_to_assets', 'capx_to_assets', 'dp_to_assets', 'emp_to_assets', 'tdc1_to_assets', 'bonus_to_assets', 'ppent_to_assets', 're_to_assets', 'rect_to_assets', 'salary_to_assets', 'option_to_asset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ef7ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using forced_3 as target variable\n",
    "# result3_exec: A DataFrame storing important variables from exec_cols, sorted by importance.\n",
    "# result3_firm: A DataFrame storing important variables from firm_cols, sorted by importance.\n",
    "'''\n",
    "result3_exec = pd.DataFrame()\n",
    "result3_firm = pd.DataFrame()\n",
    "\n",
    "top_k_features_df, fig = var_select_boosting(\n",
    "    df = df, k = len(exec_cols), X = exec_cols, y = 'forced_3')\n",
    "result3_exec['exec_all'] = top_k_features_df['feature'].tolist()\n",
    "\n",
    "top_k_features_df, fig = var_select_boosting(\n",
    "    df = df, k = len(firm_cols), X = firm_cols, y = 'forced_3')\n",
    "result3_firm['firm_all'] = top_k_features_df['feature'].tolist()\n",
    "\n",
    "for industry in df['ff17_code'].unique():\n",
    "    print(f\"Industry: {industry}\")\n",
    "    industry_df = df[df['ff17_code'] == industry]\n",
    "    if len(industry_df) < 10:\n",
    "        print(f\"Not enough data for industry {industry} to perform variable selection.\")\n",
    "        continue\n",
    "    colname_exec = f\"exec_{industry}\"\n",
    "    colname_firm = f\"firm_{industry}\"\n",
    "    top_k_features_df, fig = var_select_boosting(\n",
    "        df = industry_df, k = len(exec_cols), X = exec_cols, y = 'forced_3')\n",
    "    result3_exec[colname_exec] = top_k_features_df['feature'].tolist()\n",
    "    top_k_features_df, fig = var_select_boosting(\n",
    "        df = industry_df, k = len(firm_cols), X = firm_cols, y = 'forced_3')\n",
    "    result3_firm[colname_firm] = top_k_features_df['feature'].tolist()\n",
    "    #plt.show()  # Show the plot for each industry\n",
    "    \n",
    "top_k_features_df, fig = var_select_boosting(\n",
    "    df = df, k = len(exec_cols), end = 2008, X = exec_cols, y = 'forced_3', year_col='year')\n",
    "result3_exec['exec_before'] = top_k_features_df['feature'].tolist()\n",
    "top_k_features_df, fig = var_select_boosting(\n",
    "    df = df, k = len(firm_cols), end = 2008, X = firm_cols, y = 'forced_3', year_col='year')\n",
    "result3_firm['firm_before'] = top_k_features_df['feature'].tolist()\n",
    "top_k_features_df, fig = var_select_boosting(\n",
    "    df = df, k = len(exec_cols), start = 2009, X = exec_cols, y = 'forced_3', year_col='year')\n",
    "result3_exec['exec_after'] = top_k_features_df['feature'].tolist()\n",
    "top_k_features_df, fig = var_select_boosting(\n",
    "    df = df, k = len(firm_cols), start = 2009, X = firm_cols, y = 'forced_3', year_col='year')\n",
    "result3_firm['firm_after'] = top_k_features_df['feature'].tolist()\n",
    "\n",
    "result3_exec.to_csv(\"data/var_select_exec_3.csv\", index=False)\n",
    "result3_firm.to_csv(\"data/var_select_firm_3.csv\", index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc9e264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using forced_34 as target variable\n",
    "# result34_exec: A DataFrame storing important variables from exec_cols, sorted by importance.\n",
    "# result34_firm: A DataFrame storing important variables from firm_cols, sorted by importance.\n",
    "'''\n",
    "result34_exec = pd.DataFrame()\n",
    "result34_firm = pd.DataFrame()\n",
    "\n",
    "top_k_features_df, fig = var_select_boosting(\n",
    "    df = df, k = len(exec_cols), X = exec_cols, y = 'forced_3_4')\n",
    "result34_exec['exec_all'] = top_k_features_df['feature'].tolist()\n",
    "\n",
    "top_k_features_df, fig = var_select_boosting(\n",
    "    df = df, k = len(firm_cols), X = firm_cols, y = 'forced_3_4')\n",
    "result34_firm['firm_all'] = top_k_features_df['feature'].tolist()\n",
    "\n",
    "for industry in df['ff17_code'].unique():\n",
    "    print(f\"Industry: {industry}\")\n",
    "    industry_df = df[df['ff17_code'] == industry]\n",
    "    if len(industry_df) < 10:\n",
    "        print(f\"Not enough data for industry {industry} to perform variable selection.\")\n",
    "        continue\n",
    "    colname_exec = f\"exec_{industry}\"\n",
    "    colname_firm = f\"firm_{industry}\"\n",
    "    top_k_features_df, fig = var_select_boosting(\n",
    "        df = industry_df, k = len(exec_cols), X = exec_cols, y = 'forced_3_4')\n",
    "    result34_exec[colname_exec] = top_k_features_df['feature'].tolist()\n",
    "    top_k_features_df, fig = var_select_boosting(\n",
    "        df = industry_df, k = len(firm_cols), X = firm_cols, y = 'forced_3_4')\n",
    "    result34_firm[colname_firm] = top_k_features_df['feature'].tolist()\n",
    "    #plt.show()  # Show the plot for each industry\n",
    "    \n",
    "top_k_features_df, fig = var_select_boosting(\n",
    "    df = df, k = len(exec_cols), end = 2008, X = exec_cols, y = 'forced_3_4', year_col='year')\n",
    "result34_exec['exec_before'] = top_k_features_df['feature'].tolist()\n",
    "top_k_features_df, fig = var_select_boosting(\n",
    "    df = df, k = len(firm_cols), end = 2008, X = firm_cols, y = 'forced_3_4', year_col='year')\n",
    "result34_firm['firm_before'] = top_k_features_df['feature'].tolist()\n",
    "top_k_features_df, fig = var_select_boosting(\n",
    "    df = df, k = len(exec_cols), start = 2009, X = exec_cols, y = 'forced_3_4', year_col='year')\n",
    "result34_exec['exec_after'] = top_k_features_df['feature'].tolist()\n",
    "top_k_features_df, fig = var_select_boosting(\n",
    "    df = df, k = len(firm_cols), start = 2009, X = firm_cols, y = 'forced_3_4', year_col='year')\n",
    "result34_firm['firm_after'] = top_k_features_df['feature'].tolist()\n",
    "\n",
    "result34_exec.to_csv(\"data/var_select_exec_34.csv\", index=False)\n",
    "result34_firm.to_csv(\"data/var_select_firm_34.csv\", index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8427005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using all variables as features\n",
    "# result_all: A DataFrame storing important variables from all columns, sorted by importance.\n",
    "# before_3: A DataFrame storing important variables before 2009(financial crisis), sorted by importance.\n",
    "# after_3: A DataFrame storing important variables after 2009(financial crisis), sorted by importance.\n",
    "\n",
    "'''\n",
    "result_all = pd.DataFrame()\n",
    "\n",
    "cols = ['gender', 'page', 'roa', 'gross_margin', 'operating_margin', 'asset_turnover', 'receivables_turnover', 'inventory_turnover', 'debt_to_assets', 'equity_ratio', 'debt_to_equity', 'current_ratio', 'quick_ratio', 'working_capital', 'ocf_to_assets', 'ocf_to_sales', \n",
    "        'ebit_to_assets', 'ebitda_to_assets', 'pi_to_assets', 'ib_to_assets', 'txt_to_assets', 'xint_to_assets', 'capx_to_assets', 'dp_to_assets', 'emp_to_assets', 'tdc1_to_assets', 'bonus_to_assets', 'ppent_to_assets', 're_to_assets', 'rect_to_assets', 'salary_to_assets', 'option_to_asset', \n",
    "        'roa_3y_lag', 'gross_margin_3y_lag', 'operating_margin_3y_lag', 'asset_turnover_3y_lag', 'receivables_turnover_3y_lag', 'inventory_turnover_3y_lag', 'debt_to_assets_3y_lag', 'equity_ratio_3y_lag', 'debt_to_equity_3y_lag', 'current_ratio_3y_lag', 'quick_ratio_3y_lag', 'working_capital_3y_lag', 'ocf_to_assets_3y_lag', 'ocf_to_sales_3y_lag', \n",
    "        'ebit_to_assets_3y_lag', 'ebitda_to_assets_3y_lag', 'pi_to_assets_3y_lag', 'ib_to_assets_3y_lag', 'txt_to_assets_3y_lag', 'xint_to_assets_3y_lag', 'capx_to_assets_3y_lag', 'dp_to_assets_3y_lag', 'emp_to_assets_3y_lag', 'tdc1_to_assets_3y_lag', 'bonus_to_assets_3y_lag', 'ppent_to_assets_3y_lag', 're_to_assets_3y_lag', 'rect_to_assets_3y_lag', 'salary_to_assets_3y_lag', 'option_to_asset_3y_lag']\n",
    "\n",
    "top_k_features_df, fig = var_select_boosting(\n",
    "    df = df, k = len(cols), X = cols, y = 'forced_3')\n",
    "result_all['all_3'] = top_k_features_df['feature'].tolist()\n",
    "\n",
    "for industry in df['ff17_code'].unique():\n",
    "    print(f\"Industry: {industry}\")\n",
    "    industry_df = df[df['ff17_code'] == industry]\n",
    "    if len(industry_df) < 10:\n",
    "        print(f\"Not enough data for industry {industry} to perform variable selection.\")\n",
    "        continue\n",
    "    colname = f\"{industry}\"\n",
    "    top_k_features_df, fig = var_select_boosting(\n",
    "        df = industry_df, k = len(cols), X = cols, y = 'forced_3')\n",
    "    result_all[colname + '_3'] = top_k_features_df['feature'].tolist()\n",
    "    #plt.show()  # Show the plot for each industry\n",
    "top_k_features_df, fig = var_select_boosting(\n",
    "    df = df, k = len(cols), end = 2008, X = cols, y = 'forced_3', year_col='year')\n",
    "result_all['all_before_3'] = top_k_features_df['feature'].tolist()\n",
    "top_k_features_df, fig = var_select_boosting(\n",
    "    df = df, k = len(cols), start = 2009, X = cols, y = 'forced_3', year_col='year')\n",
    "result_all['all_after_3'] = top_k_features_df['feature'].tolist()\n",
    "\n",
    "\n",
    "top_k_features_df, fig = var_select_boosting(\n",
    "    df = df, k = len(cols), X = cols, y = 'forced_3_4')\n",
    "result_all['all_34'] = top_k_features_df['feature'].tolist()\n",
    "\n",
    "for industry in df['ff17_code'].unique():\n",
    "    print(f\"Industry: {industry}\")\n",
    "    industry_df = df[df['ff17_code'] == industry]\n",
    "    if len(industry_df) < 10:\n",
    "        print(f\"Not enough data for industry {industry} to perform variable selection.\")\n",
    "        continue\n",
    "    colname = f\"{industry}\"\n",
    "    top_k_features_df, fig = var_select_boosting(\n",
    "        df = industry_df, k = len(cols), X = cols, y = 'forced_3_4')\n",
    "    result_all[colname + '_34'] = top_k_features_df['feature'].tolist()\n",
    "\n",
    "top_k_features_df, fig = var_select_boosting(\n",
    "    df = df, k = len(cols), end = 2008, X = cols, y = 'forced_3_4', year_col='year')\n",
    "result_all['all_before_34'] = top_k_features_df['feature'].tolist()\n",
    "\n",
    "top_k_features_df, fig = var_select_boosting(\n",
    "    df = df, k = len(cols), start = 2009, X = cols, y = 'forced_3_4', year_col='year')\n",
    "result_all['all_after_34'] = top_k_features_df['feature'].tolist()\n",
    "\n",
    "result_all.to_csv(\"data/var_select_all.csv\", index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89fa1cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>all_3</th>\n",
       "      <th>13_3</th>\n",
       "      <th>17_3</th>\n",
       "      <th>11_3</th>\n",
       "      <th>6_3</th>\n",
       "      <th>7_3</th>\n",
       "      <th>15_3</th>\n",
       "      <th>9_3</th>\n",
       "      <th>16_3</th>\n",
       "      <th>3_3</th>\n",
       "      <th>...</th>\n",
       "      <th>1_34</th>\n",
       "      <th>14_34</th>\n",
       "      <th>5_34</th>\n",
       "      <th>10_34</th>\n",
       "      <th>2_34</th>\n",
       "      <th>12_34</th>\n",
       "      <th>8_34</th>\n",
       "      <th>4_34</th>\n",
       "      <th>all_before_34</th>\n",
       "      <th>all_after_34</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>roa</td>\n",
       "      <td>bonus_to_assets_3y_lag</td>\n",
       "      <td>ib_to_assets</td>\n",
       "      <td>ib_to_assets</td>\n",
       "      <td>roa</td>\n",
       "      <td>roa</td>\n",
       "      <td>roa</td>\n",
       "      <td>ocf_to_assets_3y_lag</td>\n",
       "      <td>debt_to_assets</td>\n",
       "      <td>roa</td>\n",
       "      <td>...</td>\n",
       "      <td>inventory_turnover_3y_lag</td>\n",
       "      <td>roa</td>\n",
       "      <td>debt_to_assets_3y_lag</td>\n",
       "      <td>operating_margin</td>\n",
       "      <td>ib_to_assets</td>\n",
       "      <td>ebit_to_assets_3y_lag</td>\n",
       "      <td>ocf_to_assets_3y_lag</td>\n",
       "      <td>ebit_to_assets</td>\n",
       "      <td>pi_to_assets</td>\n",
       "      <td>ib_to_assets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pi_to_assets</td>\n",
       "      <td>pi_to_assets_3y_lag</td>\n",
       "      <td>rect_to_assets_3y_lag</td>\n",
       "      <td>gross_margin_3y_lag</td>\n",
       "      <td>quick_ratio_3y_lag</td>\n",
       "      <td>inventory_turnover_3y_lag</td>\n",
       "      <td>dp_to_assets_3y_lag</td>\n",
       "      <td>re_to_assets</td>\n",
       "      <td>bonus_to_assets</td>\n",
       "      <td>ppent_to_assets_3y_lag</td>\n",
       "      <td>...</td>\n",
       "      <td>roa</td>\n",
       "      <td>gross_margin_3y_lag</td>\n",
       "      <td>quick_ratio</td>\n",
       "      <td>xint_to_assets</td>\n",
       "      <td>debt_to_assets</td>\n",
       "      <td>ebit_to_assets</td>\n",
       "      <td>operating_margin_3y_lag</td>\n",
       "      <td>bonus_to_assets</td>\n",
       "      <td>ib_to_assets</td>\n",
       "      <td>option_to_asset_3y_lag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bonus_to_assets</td>\n",
       "      <td>ocf_to_assets</td>\n",
       "      <td>pi_to_assets</td>\n",
       "      <td>pi_to_assets_3y_lag</td>\n",
       "      <td>pi_to_assets_3y_lag</td>\n",
       "      <td>debt_to_assets_3y_lag</td>\n",
       "      <td>ebit_to_assets</td>\n",
       "      <td>quick_ratio_3y_lag</td>\n",
       "      <td>xint_to_assets</td>\n",
       "      <td>salary_to_assets_3y_lag</td>\n",
       "      <td>...</td>\n",
       "      <td>bonus_to_assets_3y_lag</td>\n",
       "      <td>ocf_to_sales_3y_lag</td>\n",
       "      <td>pi_to_assets_3y_lag</td>\n",
       "      <td>current_ratio</td>\n",
       "      <td>equity_ratio</td>\n",
       "      <td>asset_turnover_3y_lag</td>\n",
       "      <td>re_to_assets_3y_lag</td>\n",
       "      <td>txt_to_assets_3y_lag</td>\n",
       "      <td>bonus_to_assets</td>\n",
       "      <td>page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ib_to_assets</td>\n",
       "      <td>ocf_to_assets_3y_lag</td>\n",
       "      <td>option_to_asset</td>\n",
       "      <td>current_ratio_3y_lag</td>\n",
       "      <td>ebitda_to_assets</td>\n",
       "      <td>xint_to_assets_3y_lag</td>\n",
       "      <td>debt_to_assets_3y_lag</td>\n",
       "      <td>page</td>\n",
       "      <td>operating_margin</td>\n",
       "      <td>ocf_to_assets</td>\n",
       "      <td>...</td>\n",
       "      <td>ebit_to_assets_3y_lag</td>\n",
       "      <td>ebitda_to_assets</td>\n",
       "      <td>roa</td>\n",
       "      <td>salary_to_assets</td>\n",
       "      <td>debt_to_assets_3y_lag</td>\n",
       "      <td>quick_ratio_3y_lag</td>\n",
       "      <td>xint_to_assets_3y_lag</td>\n",
       "      <td>emp_to_assets</td>\n",
       "      <td>page</td>\n",
       "      <td>option_to_asset</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>option_to_asset</td>\n",
       "      <td>equity_ratio_3y_lag</td>\n",
       "      <td>bonus_to_assets</td>\n",
       "      <td>ebit_to_assets_3y_lag</td>\n",
       "      <td>ppent_to_assets_3y_lag</td>\n",
       "      <td>working_capital_3y_lag</td>\n",
       "      <td>pi_to_assets</td>\n",
       "      <td>equity_ratio_3y_lag</td>\n",
       "      <td>ocf_to_assets</td>\n",
       "      <td>ebit_to_assets</td>\n",
       "      <td>...</td>\n",
       "      <td>dp_to_assets_3y_lag</td>\n",
       "      <td>pi_to_assets_3y_lag</td>\n",
       "      <td>ocf_to_assets_3y_lag</td>\n",
       "      <td>pi_to_assets</td>\n",
       "      <td>roa</td>\n",
       "      <td>re_to_assets_3y_lag</td>\n",
       "      <td>roa</td>\n",
       "      <td>salary_to_assets</td>\n",
       "      <td>capx_to_assets_3y_lag</td>\n",
       "      <td>inventory_turnover_3y_lag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>xint_to_assets_3y_lag</td>\n",
       "      <td>emp_to_assets_3y_lag</td>\n",
       "      <td>ocf_to_sales_3y_lag</td>\n",
       "      <td>dp_to_assets</td>\n",
       "      <td>ib_to_assets_3y_lag</td>\n",
       "      <td>ocf_to_assets_3y_lag</td>\n",
       "      <td>ebitda_to_assets_3y_lag</td>\n",
       "      <td>operating_margin_3y_lag</td>\n",
       "      <td>ib_to_assets_3y_lag</td>\n",
       "      <td>operating_margin_3y_lag</td>\n",
       "      <td>...</td>\n",
       "      <td>equity_ratio_3y_lag</td>\n",
       "      <td>gender</td>\n",
       "      <td>ib_to_assets_3y_lag</td>\n",
       "      <td>emp_to_assets_3y_lag</td>\n",
       "      <td>ib_to_assets_3y_lag</td>\n",
       "      <td>debt_to_equity_3y_lag</td>\n",
       "      <td>asset_turnover_3y_lag</td>\n",
       "      <td>current_ratio_3y_lag</td>\n",
       "      <td>xint_to_assets</td>\n",
       "      <td>capx_to_assets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>ocf_to_assets_3y_lag</td>\n",
       "      <td>gender</td>\n",
       "      <td>ebitda_to_assets_3y_lag</td>\n",
       "      <td>receivables_turnover</td>\n",
       "      <td>ebit_to_assets_3y_lag</td>\n",
       "      <td>pi_to_assets_3y_lag</td>\n",
       "      <td>ib_to_assets_3y_lag</td>\n",
       "      <td>debt_to_assets_3y_lag</td>\n",
       "      <td>ppent_to_assets_3y_lag</td>\n",
       "      <td>roa_3y_lag</td>\n",
       "      <td>...</td>\n",
       "      <td>receivables_turnover_3y_lag</td>\n",
       "      <td>quick_ratio_3y_lag</td>\n",
       "      <td>txt_to_assets_3y_lag</td>\n",
       "      <td>ppent_to_assets_3y_lag</td>\n",
       "      <td>ppent_to_assets_3y_lag</td>\n",
       "      <td>capx_to_assets_3y_lag</td>\n",
       "      <td>capx_to_assets_3y_lag</td>\n",
       "      <td>asset_turnover_3y_lag</td>\n",
       "      <td>txt_to_assets</td>\n",
       "      <td>equity_ratio_3y_lag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>roa_3y_lag</td>\n",
       "      <td>roa_3y_lag</td>\n",
       "      <td>capx_to_assets</td>\n",
       "      <td>equity_ratio</td>\n",
       "      <td>txt_to_assets_3y_lag</td>\n",
       "      <td>ib_to_assets_3y_lag</td>\n",
       "      <td>pi_to_assets_3y_lag</td>\n",
       "      <td>inventory_turnover_3y_lag</td>\n",
       "      <td>bonus_to_assets_3y_lag</td>\n",
       "      <td>pi_to_assets_3y_lag</td>\n",
       "      <td>...</td>\n",
       "      <td>gender</td>\n",
       "      <td>operating_margin_3y_lag</td>\n",
       "      <td>ebit_to_assets_3y_lag</td>\n",
       "      <td>re_to_assets_3y_lag</td>\n",
       "      <td>tdc1_to_assets_3y_lag</td>\n",
       "      <td>xint_to_assets_3y_lag</td>\n",
       "      <td>txt_to_assets_3y_lag</td>\n",
       "      <td>ib_to_assets_3y_lag</td>\n",
       "      <td>ocf_to_sales_3y_lag</td>\n",
       "      <td>re_to_assets_3y_lag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>xint_to_assets</td>\n",
       "      <td>ib_to_assets_3y_lag</td>\n",
       "      <td>current_ratio</td>\n",
       "      <td>roa_3y_lag</td>\n",
       "      <td>ebitda_to_assets_3y_lag</td>\n",
       "      <td>emp_to_assets_3y_lag</td>\n",
       "      <td>salary_to_assets_3y_lag</td>\n",
       "      <td>txt_to_assets_3y_lag</td>\n",
       "      <td>rect_to_assets_3y_lag</td>\n",
       "      <td>ib_to_assets_3y_lag</td>\n",
       "      <td>...</td>\n",
       "      <td>ib_to_assets_3y_lag</td>\n",
       "      <td>txt_to_assets_3y_lag</td>\n",
       "      <td>ebitda_to_assets_3y_lag</td>\n",
       "      <td>salary_to_assets_3y_lag</td>\n",
       "      <td>emp_to_assets_3y_lag</td>\n",
       "      <td>ebitda_to_assets_3y_lag</td>\n",
       "      <td>ebit_to_assets_3y_lag</td>\n",
       "      <td>ebitda_to_assets_3y_lag</td>\n",
       "      <td>ebitda_to_assets_3y_lag</td>\n",
       "      <td>asset_turnover_3y_lag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>ib_to_assets_3y_lag</td>\n",
       "      <td>rect_to_assets_3y_lag</td>\n",
       "      <td>gender</td>\n",
       "      <td>tdc1_to_assets_3y_lag</td>\n",
       "      <td>bonus_to_assets_3y_lag</td>\n",
       "      <td>salary_to_assets_3y_lag</td>\n",
       "      <td>rect_to_assets_3y_lag</td>\n",
       "      <td>tdc1_to_assets_3y_lag</td>\n",
       "      <td>salary_to_assets_3y_lag</td>\n",
       "      <td>ebitda_to_assets_3y_lag</td>\n",
       "      <td>...</td>\n",
       "      <td>salary_to_assets_3y_lag</td>\n",
       "      <td>ebit_to_assets_3y_lag</td>\n",
       "      <td>salary_to_assets_3y_lag</td>\n",
       "      <td>option_to_asset_3y_lag</td>\n",
       "      <td>salary_to_assets_3y_lag</td>\n",
       "      <td>salary_to_assets_3y_lag</td>\n",
       "      <td>pi_to_assets_3y_lag</td>\n",
       "      <td>rect_to_assets_3y_lag</td>\n",
       "      <td>gender</td>\n",
       "      <td>gender</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    all_3                    13_3                     17_3  \\\n",
       "0                     roa  bonus_to_assets_3y_lag             ib_to_assets   \n",
       "1            pi_to_assets     pi_to_assets_3y_lag    rect_to_assets_3y_lag   \n",
       "2         bonus_to_assets           ocf_to_assets             pi_to_assets   \n",
       "3            ib_to_assets    ocf_to_assets_3y_lag          option_to_asset   \n",
       "4         option_to_asset     equity_ratio_3y_lag          bonus_to_assets   \n",
       "..                    ...                     ...                      ...   \n",
       "57  xint_to_assets_3y_lag    emp_to_assets_3y_lag      ocf_to_sales_3y_lag   \n",
       "58   ocf_to_assets_3y_lag                  gender  ebitda_to_assets_3y_lag   \n",
       "59             roa_3y_lag              roa_3y_lag           capx_to_assets   \n",
       "60         xint_to_assets     ib_to_assets_3y_lag            current_ratio   \n",
       "61    ib_to_assets_3y_lag   rect_to_assets_3y_lag                   gender   \n",
       "\n",
       "                     11_3                      6_3                        7_3  \\\n",
       "0            ib_to_assets                      roa                        roa   \n",
       "1     gross_margin_3y_lag       quick_ratio_3y_lag  inventory_turnover_3y_lag   \n",
       "2     pi_to_assets_3y_lag      pi_to_assets_3y_lag      debt_to_assets_3y_lag   \n",
       "3    current_ratio_3y_lag         ebitda_to_assets      xint_to_assets_3y_lag   \n",
       "4   ebit_to_assets_3y_lag   ppent_to_assets_3y_lag     working_capital_3y_lag   \n",
       "..                    ...                      ...                        ...   \n",
       "57           dp_to_assets      ib_to_assets_3y_lag       ocf_to_assets_3y_lag   \n",
       "58   receivables_turnover    ebit_to_assets_3y_lag        pi_to_assets_3y_lag   \n",
       "59           equity_ratio     txt_to_assets_3y_lag        ib_to_assets_3y_lag   \n",
       "60             roa_3y_lag  ebitda_to_assets_3y_lag       emp_to_assets_3y_lag   \n",
       "61  tdc1_to_assets_3y_lag   bonus_to_assets_3y_lag    salary_to_assets_3y_lag   \n",
       "\n",
       "                       15_3                        9_3  \\\n",
       "0                       roa       ocf_to_assets_3y_lag   \n",
       "1       dp_to_assets_3y_lag               re_to_assets   \n",
       "2            ebit_to_assets         quick_ratio_3y_lag   \n",
       "3     debt_to_assets_3y_lag                       page   \n",
       "4              pi_to_assets        equity_ratio_3y_lag   \n",
       "..                      ...                        ...   \n",
       "57  ebitda_to_assets_3y_lag    operating_margin_3y_lag   \n",
       "58      ib_to_assets_3y_lag      debt_to_assets_3y_lag   \n",
       "59      pi_to_assets_3y_lag  inventory_turnover_3y_lag   \n",
       "60  salary_to_assets_3y_lag       txt_to_assets_3y_lag   \n",
       "61    rect_to_assets_3y_lag      tdc1_to_assets_3y_lag   \n",
       "\n",
       "                       16_3                      3_3  ...  \\\n",
       "0            debt_to_assets                      roa  ...   \n",
       "1           bonus_to_assets   ppent_to_assets_3y_lag  ...   \n",
       "2            xint_to_assets  salary_to_assets_3y_lag  ...   \n",
       "3          operating_margin            ocf_to_assets  ...   \n",
       "4             ocf_to_assets           ebit_to_assets  ...   \n",
       "..                      ...                      ...  ...   \n",
       "57      ib_to_assets_3y_lag  operating_margin_3y_lag  ...   \n",
       "58   ppent_to_assets_3y_lag               roa_3y_lag  ...   \n",
       "59   bonus_to_assets_3y_lag      pi_to_assets_3y_lag  ...   \n",
       "60    rect_to_assets_3y_lag      ib_to_assets_3y_lag  ...   \n",
       "61  salary_to_assets_3y_lag  ebitda_to_assets_3y_lag  ...   \n",
       "\n",
       "                           1_34                    14_34  \\\n",
       "0     inventory_turnover_3y_lag                      roa   \n",
       "1                           roa      gross_margin_3y_lag   \n",
       "2        bonus_to_assets_3y_lag      ocf_to_sales_3y_lag   \n",
       "3         ebit_to_assets_3y_lag         ebitda_to_assets   \n",
       "4           dp_to_assets_3y_lag      pi_to_assets_3y_lag   \n",
       "..                          ...                      ...   \n",
       "57          equity_ratio_3y_lag                   gender   \n",
       "58  receivables_turnover_3y_lag       quick_ratio_3y_lag   \n",
       "59                       gender  operating_margin_3y_lag   \n",
       "60          ib_to_assets_3y_lag     txt_to_assets_3y_lag   \n",
       "61      salary_to_assets_3y_lag    ebit_to_assets_3y_lag   \n",
       "\n",
       "                       5_34                    10_34                     2_34  \\\n",
       "0     debt_to_assets_3y_lag         operating_margin             ib_to_assets   \n",
       "1               quick_ratio           xint_to_assets           debt_to_assets   \n",
       "2       pi_to_assets_3y_lag            current_ratio             equity_ratio   \n",
       "3                       roa         salary_to_assets    debt_to_assets_3y_lag   \n",
       "4      ocf_to_assets_3y_lag             pi_to_assets                      roa   \n",
       "..                      ...                      ...                      ...   \n",
       "57      ib_to_assets_3y_lag     emp_to_assets_3y_lag      ib_to_assets_3y_lag   \n",
       "58     txt_to_assets_3y_lag   ppent_to_assets_3y_lag   ppent_to_assets_3y_lag   \n",
       "59    ebit_to_assets_3y_lag      re_to_assets_3y_lag    tdc1_to_assets_3y_lag   \n",
       "60  ebitda_to_assets_3y_lag  salary_to_assets_3y_lag     emp_to_assets_3y_lag   \n",
       "61  salary_to_assets_3y_lag   option_to_asset_3y_lag  salary_to_assets_3y_lag   \n",
       "\n",
       "                      12_34                     8_34                     4_34  \\\n",
       "0     ebit_to_assets_3y_lag     ocf_to_assets_3y_lag           ebit_to_assets   \n",
       "1            ebit_to_assets  operating_margin_3y_lag          bonus_to_assets   \n",
       "2     asset_turnover_3y_lag      re_to_assets_3y_lag     txt_to_assets_3y_lag   \n",
       "3        quick_ratio_3y_lag    xint_to_assets_3y_lag            emp_to_assets   \n",
       "4       re_to_assets_3y_lag                      roa         salary_to_assets   \n",
       "..                      ...                      ...                      ...   \n",
       "57    debt_to_equity_3y_lag    asset_turnover_3y_lag     current_ratio_3y_lag   \n",
       "58    capx_to_assets_3y_lag    capx_to_assets_3y_lag    asset_turnover_3y_lag   \n",
       "59    xint_to_assets_3y_lag     txt_to_assets_3y_lag      ib_to_assets_3y_lag   \n",
       "60  ebitda_to_assets_3y_lag    ebit_to_assets_3y_lag  ebitda_to_assets_3y_lag   \n",
       "61  salary_to_assets_3y_lag      pi_to_assets_3y_lag    rect_to_assets_3y_lag   \n",
       "\n",
       "              all_before_34               all_after_34  \n",
       "0              pi_to_assets               ib_to_assets  \n",
       "1              ib_to_assets     option_to_asset_3y_lag  \n",
       "2           bonus_to_assets                       page  \n",
       "3                      page            option_to_asset  \n",
       "4     capx_to_assets_3y_lag  inventory_turnover_3y_lag  \n",
       "..                      ...                        ...  \n",
       "57           xint_to_assets             capx_to_assets  \n",
       "58            txt_to_assets        equity_ratio_3y_lag  \n",
       "59      ocf_to_sales_3y_lag        re_to_assets_3y_lag  \n",
       "60  ebitda_to_assets_3y_lag      asset_turnover_3y_lag  \n",
       "61                   gender                     gender  \n",
       "\n",
       "[62 rows x 40 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_all = pd.read_csv(\"data/var_select_all.csv\")\n",
    "result_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed9cb0f",
   "metadata": {},
   "source": [
    "# tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11378401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def find_optimal_tenure_bins_xgb(df_input: pd.DataFrame,\n",
    "                                           tenure_col: str = 'tenure',\n",
    "                                           target_col: str = 'forced_3',\n",
    "                                           max_tree_depth: int = 3,\n",
    "                                           random_state: int = 42) -> list:\n",
    "    \"\"\"\n",
    "    Suggests bins for a tenure column using an XGBoost model with a single tree.\n",
    "    It extracts the split points for the tenure variable from the XGBoost tree structure.\n",
    "    \"\"\"\n",
    "    df_processed = df_input.copy()\n",
    "\n",
    "    # Remove rows with NaN in essential columns.\n",
    "    df_processed.dropna(subset=[tenure_col, target_col], inplace=True)\n",
    "\n",
    "    X = df_processed[[tenure_col]]\n",
    "    y_original = df_processed[target_col]\n",
    "    \n",
    "    # Transform the target variable using LabelEncoder.\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y_original)\n",
    "\n",
    "    # Initialize and train the XGBoost model\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=1,             # Use only one tree\n",
    "        max_depth=max_tree_depth,\n",
    "        random_state=random_state,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    )\n",
    "    model.fit(X, y) # An error may occur here if X or y are unsuitable.\n",
    "\n",
    "    # Extract split point information from the trained tree\n",
    "    booster = model.get_booster()\n",
    "    tree_df = booster.trees_to_dataframe() # May error depending on the model structure.\n",
    "\n",
    "    # Extract split values corresponding to tenure_col\n",
    "    tenure_splits = tree_df[tree_df['Feature'] == tenure_col]['Split'].dropna().unique()\n",
    "    tenure_splits.sort()\n",
    "\n",
    "    min_val = df_processed[tenure_col].min() # Error if df_processed is empty\n",
    "    max_val = df_processed[tenure_col].max()\n",
    "\n",
    "    if len(tenure_splits) == 0:\n",
    "        # If there are no splits for tenure_col, return the entire range as a single bin.\n",
    "        return [min_val, max_val + 1e-9 if min_val == max_val else max_val]\n",
    "\n",
    "    # Create the final bin edges\n",
    "    bin_edges = sorted(list(set([min_val] + list(tenure_splits) + [max_val])))\n",
    "\n",
    "    # Check and adjust bin edge validity (for pd.cut)\n",
    "    if len(bin_edges) < 2:\n",
    "        return [min_val, max_val + 1e-9 if min_val == max_val else max_val]\n",
    "    if len(bin_edges) == 2 and bin_edges[0] == bin_edges[1]: # If all tenure values are the same\n",
    "        bin_edges = [bin_edges[0], bin_edges[1] + 1e-9]\n",
    "\n",
    "    return bin_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80fd406",
   "metadata": {},
   "source": [
    "## 3,4로 할지? 몇개 정도로 하면 좋을지 논의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2496c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결정된 Tenure 구간 경계 (XGBoost 기반, 오류 처리 제거): [np.int64(1), np.float64(2.0), np.float64(7.0), np.float64(10.0), np.float64(17.0), np.float64(21.0), np.float64(46.0), np.int64(62)]\n",
      "\n",
      "Tenure 구간별 빈도수 (XGBoost 기반, 오류 처리 제거):\n",
      "tenure_binned_xgb\n",
      "(0.999, 2.0]     8074\n",
      "(2.0, 7.0]      15866\n",
      "(7.0, 10.0]      5248\n",
      "(10.0, 17.0]     5617\n",
      "(17.0, 21.0]     1258\n",
      "(21.0, 46.0]     1839\n",
      "(46.0, 62.0]       43\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/deep_env/lib/python3.11/site-packages/xgboost/training.py:183: UserWarning: [14:12:58] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "# 1. Find optimal tenure bins using XGBoost\n",
    "#    (An error will occur here if df is missing, column names are incorrect, or the data is unsuitable.)\n",
    "tenure_bin_edges_xgb = find_optimal_tenure_bins_xgb(\n",
    "    df_input=df, \n",
    "    tenure_col='tenure',\n",
    "    target_col='forced_3',\n",
    "    max_tree_depth=3,  # Example: max tree depth of 3\n",
    "    random_state=42    # random_state for reproducibility\n",
    ")\n",
    "print(f\"Determined Tenure bin edges (XGBoost-based, error handling removed): {tenure_bin_edges_xgb}\")\n",
    "\n",
    "# 2. Add the new binned tenure column and check the results\n",
    "#    (Assuming tenure_bin_edges_xgb is returned in a format suitable for pd.cut)\n",
    "'''\n",
    "df['tenure_binned_xgb'] = pd.cut(\n",
    "    df['tenure'],\n",
    "    bins=tenure_bin_edges_xgb, # This list must have at least two elements for pd.cut to work correctly\n",
    "    right=True,\n",
    "    include_lowest=True,\n",
    "    duplicates='drop' # Handles duplicate bin edges automatically (a safeguard in pd.cut)\n",
    ")\n",
    "\n",
    "print(\"\\nFrequency count per Tenure bin (XGBoost-based, error handling removed):\")\n",
    "print(df['tenure_binned_xgb'].value_counts().sort_index())\n",
    "\n",
    "# 3. (Optional) Create dummy variables\n",
    "#tenure_dummies_xgb = pd.get_dummies(df['tenure_binned_xgb'], prefix='tenure_bin_xgb_s', dummy_na=False)\n",
    "#print(\"\\nTenure dummy variables (XGBoost-based, error handling removed, top 5 rows):\")\n",
    "#print(tenure_dummies_xgb.head())\n",
    "#print(\"\\n'tenure_dummies_xgb' can now be used as features in a model.\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ccc6ca",
   "metadata": {},
   "source": [
    "(0.999, 2.0]     8074  \n",
    "(2.0, 10.0]     21114  \n",
    "(10.0, 21.0]     6875  \n",
    "(21.0, 62.0]     1882  \n",
    "  \n",
    "(0.999, 2.0]     8074  \n",
    "(2.0, 7.0]      15866  \n",
    "(7.0, 10.0]      5248  \n",
    "(10.0, 17.0]     5617  \n",
    "(17.0, 21.0]     1258  \n",
    "(21.0, 46.0]     1839  \n",
    "(46.0, 62.0]       43  \n",
    "  \n",
    "depth3 is more uniformly distributed, so use depth3 to make tenure indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd3d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tenure_indicator_columns(df, tenure_column_name, bin_edges):\n",
    "    df_copy = df.copy() \n",
    "    \n",
    "    if not isinstance(bin_edges, list) or len(bin_edges) < 2:\n",
    "        raise ValueError(\"bin_edges must be a list with at least 2 values.\")\n",
    "\n",
    "    # Create an indicator column for each interval\n",
    "    for i in range(len(bin_edges) - 1):\n",
    "        lower_bound = bin_edges[i]\n",
    "        upper_bound = bin_edges[i+1]\n",
    "        \n",
    "        new_col_name = f'I{tenure_column_name}{i+1}' # Set the new column name format, e.g., Itenure1\n",
    "        \n",
    "        condition = False # Initialize condition\n",
    "        if i == 0:  # For the first indicator column\n",
    "            # Change condition to be inclusive of the lower_bound (tenure >= lower_bound)\n",
    "            condition = (df_copy[tenure_column_name] >= lower_bound) & \\\n",
    "                        (df_copy[tenure_column_name] <= upper_bound)\n",
    "        else:  # For all other indicator columns\n",
    "            # Keep the original condition (tenure > lower_bound)\n",
    "            condition = (df_copy[tenure_column_name] > lower_bound) & \\\n",
    "                        (df_copy[tenure_column_name] <= upper_bound)\n",
    "            \n",
    "        df_copy[new_col_name] = condition.astype(int)\n",
    "        \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f580c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges_list = tenure_bin_edges_xgb\n",
    "df = create_tenure_indicator_columns(df, 'tenure', bin_edges_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a85ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.int64(1),\n",
       " np.float64(2.0),\n",
       " np.float64(10.0),\n",
       " np.float64(21.0),\n",
       " np.int64(62)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_edges_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e1cc85",
   "metadata": {},
   "source": [
    "# traintest flag\n",
    "3 sets each for (forced:not forced) 1:1 and 1:2 ratios, using forced_3 and forced_3_4 as target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13bfb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matched_sampling(df: pd.DataFrame, target_col: str, ratio: int, \n",
    "                                               test_ratio: float = 0.2, \n",
    "                                               random_state: int = None) -> pd.DataFrame:\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs matched case-control sampling and adds train/test indicator columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        target_col (str): The name of the classification target column.\n",
    "        ratio (int): The sampling ratio for train:test, expressed as 'ratio:1'. \n",
    "                    (e.g., ratio=1 for a 1:1 split, ratio=2 for a 2:1 split).\n",
    "        test_ratio (float, optional): The proportion of the entire dataset to allocate to the test set. \n",
    "                                    Defaults to 0.2.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with 'train' and 'test' columns added.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # 1. Case (target_col == 1)와 Control (target_col == 0) 분리\n",
    "    df_case = df_copy[df_copy[target_col] == 1]\n",
    "    df_control = df_copy[df_copy[target_col] == 0]\n",
    "\n",
    "    n_case = len(df_case)\n",
    "    n_control_needed = n_case * ratio\n",
    "\n",
    "    if n_control_needed > len(df_control):\n",
    "        print(f\"Warning: 요청된 control 샘플 수({n_control_needed})가 실제 control 샘플 수({len(df_control)})보다 많습니다. 가능한 모든 control 샘플을 사용합니다.\")\n",
    "        n_control_to_sample = len(df_control)\n",
    "    else:\n",
    "        n_control_to_sample = n_control_needed\n",
    "        \n",
    "    # 2. Control 샘플링\n",
    "    df_control_sampled = df_control.sample(n=n_control_to_sample, random_state=random_state)\n",
    "\n",
    "    # 3. 샘플링된 데이터 합치기\n",
    "    df_sampled = pd.concat([df_case, df_control_sampled])\n",
    "\n",
    "    # 4. Train/Test 분리 (샘플링된 데이터 내에서)\n",
    "    if not df_sampled.empty:\n",
    "        # 인덱스를 기준으로 분리하기 위해 인덱스만 사용\n",
    "        sampled_indices = df_sampled.index\n",
    "        \n",
    "        # target_col을 기준으로 계층적 분할(stratify)을 고려할 수 있으나,\n",
    "        # case-control 샘플링으로 이미 비율이 조절되었으므로 단순 분할도 가능합니다.\n",
    "        # 여기서는 df_sampled[target_col]을 사용하여 계층적 분할을 시도합니다.\n",
    "        # 만약 df_sampled가 매우 작아서 계층화가 불가능한 경우를 대비해 예외처리\n",
    "        try:\n",
    "            train_indices, test_indices = train_test_split(\n",
    "                sampled_indices,\n",
    "                test_size=test_ratio,\n",
    "                random_state=random_state,\n",
    "                stratify=df_sampled[target_col] if len(df_sampled[target_col].unique()) > 1 else None\n",
    "            )\n",
    "        except ValueError: # stratify가 불가능한 경우 (예: 한 클래스만 있거나 샘플이 너무 적을 때)\n",
    "             train_indices, test_indices = train_test_split(\n",
    "                sampled_indices,\n",
    "                test_size=test_ratio,\n",
    "                random_state=random_state\n",
    "            )\n",
    "\n",
    "    else: # 샘플링된 데이터가 없는 경우 (예: case가 없는 경우)\n",
    "        train_indices = pd.Index([])\n",
    "        test_indices = pd.Index([])\n",
    "\n",
    "    # 5. 'train'과 'test' 컬럼 초기화 (모두 0으로)\n",
    "    df_copy['train'] = 0\n",
    "    df_copy['test'] = 0\n",
    "\n",
    "    # 6. 샘플링된 데이터에 따라 'train', 'test' 컬럼 값 업데이트\n",
    "    df_copy.loc[train_indices, 'train'] = 1\n",
    "    df_copy.loc[test_indices, 'test'] = 1\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180ae6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def matched_sampling_multiple(df: pd.DataFrame, \n",
    "                                   target_col: str, \n",
    "                                   ratio: int, \n",
    "                                   num_samplings: int, # Specifies how many times to repeat\n",
    "                                   test_ratio: float = 0.2, \n",
    "                                   random_state: int = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    A function that performs matched case-control sampling and train/test splitting\n",
    "    multiple times for an imbalanced target column, and adds train/test columns for each iteration.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        target_col (str): The name of the classification target column.\n",
    "        ratio (int): The sampling ratio of (count of target_col == 0) / (count of target_col == 1).\n",
    "                     (e.g., ratio=2 means sampling at 1:2)\n",
    "        num_samplings (int): The number of times to repeat the sampling and splitting.\n",
    "        test_ratio (float, optional): The proportion of the test set from the total sampled data. \n",
    "                                      Defaults to 0.2.\n",
    "        random_state (int, optional): random_state for reproducibility. \n",
    "                                      Each iteration gets a different random_state based on this value. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The original DataFrame with 'trainX' and 'testX' columns added for each iteration.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # 1. Separate Case (target_col == 1) and Control (target_col == 0) (performed only once)\n",
    "    df_case = df_copy[df_copy[target_col] == 1]\n",
    "    df_control = df_copy[df_copy[target_col] == 0]\n",
    "\n",
    "    n_case = len(df_case)\n",
    "    \n",
    "    if n_case == 0:\n",
    "        print(\"Warning: No Case samples (target_col == 1) found. Cannot proceed with sampling.\")\n",
    "        # Initialize all requested train/test columns to 0\n",
    "        for i in range(1, num_samplings + 1):\n",
    "            df_copy[f'train{i}'] = 0\n",
    "            df_copy[f'test{i}'] = 0\n",
    "        return df_copy\n",
    "\n",
    "    n_control_needed = n_case * ratio\n",
    "    actual_n_control = len(df_control)\n",
    "\n",
    "    if n_control_needed > actual_n_control:\n",
    "        print(f\"Warning: For all iterations, the requested number of control samples ({n_control_needed}) is greater than the available number of control samples ({actual_n_control}). Using all available control samples.\")\n",
    "        n_control_to_sample_for_each_iteration = actual_n_control\n",
    "    else:\n",
    "        n_control_to_sample_for_each_iteration = n_control_needed\n",
    "    \n",
    "    if ratio > 0 and actual_n_control == 0:\n",
    "        print(\"Warning: No Control samples (target_col == 0) exist, but ratio is greater than 0. Splitting will proceed with Case samples only.\")\n",
    "        n_control_to_sample_for_each_iteration = 0\n",
    "\n",
    "\n",
    "    # Repeat sampling and splitting multiple times\n",
    "    for i in range(1, num_samplings + 1):\n",
    "        current_iter_random_state = None\n",
    "        if random_state is not None:\n",
    "            # Use a different random_state for each iteration, while keeping the entire process reproducible\n",
    "            current_iter_random_state = random_state + (i - 1) \n",
    "\n",
    "        # 2. Sample Controls (newly sampled from the original control pool in each iteration)\n",
    "        if n_control_to_sample_for_each_iteration > 0 and not df_control.empty:\n",
    "            df_control_sampled = df_control.sample(n=n_control_to_sample_for_each_iteration, \n",
    "                                                   random_state=current_iter_random_state)\n",
    "            # 3. Combine the sampled data\n",
    "            df_sampled_iter = pd.concat([df_case, df_control_sampled])\n",
    "        elif not df_case.empty: # If control sampling is not needed (ratio=0) or not possible (no controls), use Case samples only\n",
    "            df_sampled_iter = df_case.copy()\n",
    "            if n_control_needed > 0 and df_control.empty and i==1: # This warning is printed only once\n",
    "                 print(f\"No Control samples found. Proceeding with train/test split using Case samples only.\")\n",
    "        else: # The case of no Cases is already handled above\n",
    "            df_sampled_iter = pd.DataFrame()\n",
    "\n",
    "\n",
    "        # Generate train/test column names for the current iteration\n",
    "        train_col_name = f'train{i}'\n",
    "        test_col_name = f'test{i}'\n",
    "        \n",
    "        # Initialize columns\n",
    "        df_copy[train_col_name] = 0\n",
    "        df_copy[test_col_name] = 0\n",
    "\n",
    "        if not df_sampled_iter.empty:\n",
    "            sampled_indices_iter = df_sampled_iter.index\n",
    "            \n",
    "            stratify_on = None\n",
    "            if len(df_sampled_iter[target_col].unique()) > 1 and len(df_sampled_iter) >= 2 : # Stratification requires at least 2 samples and 2 classes\n",
    "                 # Also need to consider if each class has enough samples for the split size (handled internally by train_test_split)\n",
    "                try:\n",
    "                    # Temporarily check value_counts to more reliably determine if stratification is possible\n",
    "                    vc = df_sampled_iter[target_col].value_counts()\n",
    "                    if all(c >= 2 for c in vc): # At least 2 classes, with at least 2 samples per class (more strictly related to n_splits)\n",
    "                         stratify_on = df_sampled_iter[target_col]\n",
    "                    # else: stratify_on remains None\n",
    "                except: # General exception handling for complex situations\n",
    "                    pass # stratify_on remains None\n",
    "\n",
    "            try:\n",
    "                train_indices, test_indices = train_test_split(\n",
    "                    sampled_indices_iter,\n",
    "                    test_size=test_ratio,\n",
    "                    random_state=current_iter_random_state,\n",
    "                    stratify=stratify_on\n",
    "                )\n",
    "            except ValueError: # If stratification is not possible (e.g., only one class or too few samples)\n",
    "                 train_indices, test_indices = train_test_split(\n",
    "                    sampled_indices_iter,\n",
    "                    test_size=test_ratio,\n",
    "                    random_state=current_iter_random_state\n",
    "                )\n",
    "            \n",
    "            # Update column values\n",
    "            df_copy.loc[train_indices, train_col_name] = 1\n",
    "            df_copy.loc[test_indices, test_col_name] = 1\n",
    "        else:\n",
    "            # This case occurs when df_case is empty and is already handled at the beginning of the function.\n",
    "            # Theoretically, this part should not be reached.\n",
    "            print(f\"Iteration {i}: No data was sampled, so columns {train_col_name} and {test_col_name} cannot be populated.\")\n",
    "            \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d6dc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result12 = matched_sampling_multiple(df, target_col='forced_3', ratio = 2, num_samplings=3, test_ratio=0.2, random_state=42)\n",
    "df_result12.to_csv('data/matched_sampling_1:2.csv', index=False)\n",
    "df_result11 = matched_sampling_multiple(df, target_col='forced_3', ratio = 1, num_samplings=3, test_ratio=0.2, random_state=42)\n",
    "df_result11.to_csv('data/matched_sampling_1:1.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
